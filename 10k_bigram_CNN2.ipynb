{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Embedding, merge, Conv2D, MaxPool2D, Dropout, Merge\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_us_o=pd.read_csv('dump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(idx):\n",
    "    \"\"\"\n",
    "    encoding categorical numbers to binary\n",
    "    \"\"\"\n",
    "    y = np.zeros((len(idx),max(idx)+1))\n",
    "    y[np.arange(len(idx)), idx] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def one_hot_decoding(mtx):\n",
    "    \"\"\"\n",
    "    decoding binary to categorical numbers\n",
    "    \"\"\"\n",
    "    y = np.nonzero(mtx)[1]\n",
    "    return y\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\s+\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def text2ngrams(texts, n=2):\n",
    "    sen = []\n",
    "    sens = []\n",
    "    for sentence in texts:\n",
    "        for i in range(len(sentence)-n+1):\n",
    "            sen += [sentence[i:i+n]]\n",
    "        sens.append(sen)\n",
    "        sen = []\n",
    "    return sens\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    sentences, labels = load_data_and_labels()\n",
    "    sentences_padded = pad_sentences(sentences)\n",
    "    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "    return [x, y, vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_us = data_us_o.rename(columns={'data.description':'DATA_DESCRIPTION'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Insight & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA_DESCRIPTION\n",
    "\n",
    "d_desc = [clean_str(s) for s in data_us.DATA_DESCRIPTION.values[:10000]] # get rid of specioal characters\n",
    "d_desc = text2ngrams(d_desc, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1955, 1955)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_desc_padded = pad_sentences(d_desc)\n",
    "vocabulary, vocabulary_inv = build_vocab(d_desc_padded)\n",
    "\n",
    "# pad shorter sentences to the same size\n",
    "data_feature = np.array([[vocabulary[word] for word in sentence] for sentence in d_desc_padded])\n",
    "len(vocabulary),len(vocabulary_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target/category/label\n",
    "data_us_o.target.value_counts() # categorical data\n",
    "data_us[\"target\"] = data_us[\"target\"].astype('category')\n",
    "data_us[\"target\"] = data_us[\"target\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 49), (10000, 22))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = one_hot_encoding(data_us.target.values[:10000])\n",
    "data_feature.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into train and test set\n",
    "\n",
    "train_x, test_x, train_y, test_y \\\n",
    "= train_test_split(data_feature,y,test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9000, 49), (1000, 49), (9000, 22), (1000, 22))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, test_x.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_58 (Embedding)     (None, 49, 256)           500480    \n",
      "_________________________________________________________________\n",
      "reshape_58 (Reshape)         (None, 49, 256, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 49, 256, 2)        1538      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_59 (MaxPooling (None, 3, 256, 2)         0         \n",
      "=================================================================\n",
      "Total params: 502,018\n",
      "Trainable params: 502,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_59 (Embedding)     (None, 49, 256)           500480    \n",
      "_________________________________________________________________\n",
      "reshape_59 (Reshape)         (None, 49, 256, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 49, 256, 2)        2050      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_60 (MaxPooling (None, 4, 256, 2)         0         \n",
      "=================================================================\n",
      "Total params: 502,530\n",
      "Trainable params: 502,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_60 (Embedding)     (None, 49, 256)           500480    \n",
      "_________________________________________________________________\n",
      "reshape_60 (Reshape)         (None, 49, 256, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 49, 256, 2)        2562      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_61 (MaxPooling (None, 5, 256, 2)         0         \n",
      "=================================================================\n",
      "Total params: 503,042\n",
      "Trainable params: 503,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_15 (Merge)             (None, 12, 256, 2)        0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 22)                135190    \n",
      "=================================================================\n",
      "Total params: 1,642,780\n",
      "Trainable params: 1,642,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda2/envs/evn27/lib/python2.7/site-packages/ipykernel_launcher.py:41: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "sequence_length = data_feature.shape[1] # 49\n",
    "vocabulary_size = len(vocabulary_inv) # 1955\n",
    "embedding_dim = 256\n",
    "filter_sizes = [3,4,5]\n",
    "dropout = 0.5\n",
    "\n",
    "# Input layer for 3\n",
    "filter_size_3 = Sequential()\n",
    "filter_size_3.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length = sequence_length))\n",
    "filter_size_3.add(Reshape((sequence_length, embedding_dim, 1)))\n",
    "filter_size_3.add(Conv2D(2, kernel_size=(filter_sizes[0],embedding_dim), strides=(1,1), padding='same', activation='relu', input_shape = (sequence_length, embedding_dim, 1)))\n",
    "filter_size_3.add(MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1)))\n",
    "\n",
    "# Input layer for 4\n",
    "filter_size_4 = Sequential()\n",
    "filter_size_4.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length = sequence_length))\n",
    "filter_size_4.add(Reshape((sequence_length, embedding_dim, 1)))\n",
    "filter_size_4.add(Conv2D(2, kernel_size=(filter_sizes[1],embedding_dim), strides=(1,1), padding='same', activation='relu', input_shape = (sequence_length, embedding_dim, 1)))\n",
    "filter_size_4.add(MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1)))\n",
    "\n",
    "\n",
    "# Input layer for 5\n",
    "filter_size_5 = Sequential()\n",
    "filter_size_5.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length = sequence_length))\n",
    "filter_size_5.add(Reshape((sequence_length, embedding_dim, 1)))\n",
    "filter_size_5.add(Conv2D(2, kernel_size=(filter_sizes[2],embedding_dim), strides=(1,1), padding='same', activation='relu', input_shape = (sequence_length, embedding_dim, 1)))\n",
    "filter_size_5.add(MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([filter_size_3, filter_size_4, filter_size_5], mode='concat', concat_axis=1))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(22, activation='softmax'))\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy')\n",
    "filter_size_3.summary()\n",
    "filter_size_4.summary()\n",
    "filter_size_5.summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_x, train_y, epochs=10, verbose=2)  # batch_size=batch_size, starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_x, test_y, batch_size=256, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# predict_y = model.predict(test_x)\n",
    "predict_y_c = model.predict_classes(test_x)\n",
    "y_true = one_hot_decoding(test_y)\n",
    "y_pred = predict_y_c\n",
    "\n",
    "print('Prec/Recall/F1-Score: ', precision_recall_fscore_support(y_true+1, y_pred+1, average='micro'))\n",
    "\n",
    "CM = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "df_cm = pd.DataFrame(CM, range(max(y_true)+1), range(max(y_true)+1))\n",
    "#plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.6)#for label size\n",
    "plt.figure(figsize=(30, 30))\n",
    "sn.heatmap(df_cm, cmap=\"YlGnBu\", annot=True,annot_kws={\"size\": 16})# font size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
